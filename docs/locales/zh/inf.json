[
  {
    "id": "Llama.cpp",
    "title": "Llama.cpp - 适用于任何设备的轻量快速人工智能",
    "description": "Llama.cpp是一个由Georgi Gerganov创建并由GGML社区维护的开源项目。它允许您在计算机上直接高效运行人工智能模型，无需依赖互联网或外部服务器。\n\nLlama.cpp的优势在于轻量和便携：可在Windows、Linux、macOS甚至移动设备上运行。它使用C/C++编写，以确保速度和与不同类型硬件的兼容性，从简单的机器到高级GPU。\n\n得益于其量化技术，Llama.cpp可以在不损失太多质量的情况下减小模型大小。这意味着您可以根据设备的能力，在更小更快的版本或更大更精确的模型之间进行选择。\n\n它非常适合那些希望在本地运行人工智能、注重隐私、低资源消耗和高性能的人，无论是个人项目、研究还是专业应用。",
    "footer": "基于Llama.cpp / GGML存储库的官方文档"
  },
  {
    "id": "downloads",
    "title": "下载 - LaPlace",
    "description": "LaPlace正在开发中，很快将可供下载。我们的目标是提供一个将多个人工智能模型集中在一处的应用程序，完全可定制且易于使用。\n\n当前状态：\n• 应用程序界面：已完成\n• 翻译系统：已准备好\n• 后端：开发中\n• 集成的人工智能模型：不断发展中\n\nLaPlace的期望：\n• 在本地运行人工智能模型，无需依赖互联网\n• 即使在中等配置的机器上也能保持速度和效率\n• 为不同领域集成工作环境（开发、渗透测试、自动化等）\n• 开源应用程序，与社区共同成长\n\n如果您是开发人员、研究人员或人工智能爱好者，可以通过我们的GitHub关注项目并做出贡献。参与的人越多，LaPlace就能越快完成。",
    "footer": "© 2025 LaPlace - 面向所有人的开源人工智能"
  }
]
