[
  {
    "id": "Llama.cpp",
    "title": "Llama.cpp - Lightweight, Fast AI for Any Device",
    "description": "Llama.cpp is an open source project created by Georgi Gerganov and maintained by the GGML community. It allows AI models to run efficiently right on your computer, without needing the internet or external servers.\n\nThe big advantage of Llama.cpp is being lightweight and portable: it runs on Windows, Linux, macOS, and even on mobile devices. Written in C/C++, it ensures speed and compatibility with different hardware, from simple machines to advanced GPUs.\n\nThanks to its quantization technology, Llama.cpp can shrink model size while preserving quality. That means you can choose between smaller, faster versions or larger, more accurate ones, depending on your device.\n\nIt’s perfect for those who want local AI with privacy, low resource usage, and high performance — for personal projects, research, or even professional applications.",
    "footer": "Based on the official Llama.cpp / GGML repository documentation"
  },
  {
    "id": "downloads",
    "title": "Downloads - LaPlace",
    "description": "LaPlace is under development and will soon be available for download. Our goal is to provide an app that brings together multiple AI models in one place — customizable and easy to use.\n\nCurrent Status:\n• App interface: completed\n• Translation system: ready\n• Backend: in development\n• AI models: in progress\n\nWhat to expect from LaPlace:\n• Run AI models locally, without internet dependency\n• Speed and efficiency even on mid-range computers\n• Workspaces tailored for different areas (development, pentesting, automation, etc.)\n• Open source app that grows with the community\n\nIf you are a developer, researcher, or AI enthusiast, you can follow the project and contribute on GitHub. The more people join, the faster LaPlace becomes complete.",
    "footer": "© 2025 LaPlace - Open Source AI for Everyone"
  }
]
